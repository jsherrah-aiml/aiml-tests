{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIML machine learning test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsherrah-aiml/aiml-tests/blob/master/AIML_machine_learning_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvDBY2p1mRyB",
        "colab_type": "text"
      },
      "source": [
        "# AIML machine learning test \n",
        "\n",
        "In this exercise you will improve the test set accuracy of a deep neural network classifier on the [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) data set.  You may use any means at your disposal, but:\n",
        "* you may NOT train on the test data\n",
        "* the model must be trained in your code; you cannot simply download a model and use it for inference\n",
        "\n",
        "The desired outputs of your answer are:\n",
        "* Runnable code in a colab notebook\n",
        "* Test set accuracy before your changes (as per the code below), in %\n",
        "* Test set accuracy after your changes, in %\n",
        "* A brief description of your approach\n",
        "\n",
        "Please use the code below as a starting point.  It loads the CIFAR-100 data and trains a ResNet-18 CNN on it.\n",
        "\n",
        "Email results to Jamie Sherrah at jamie.sherrah@adelaide.edu.au.  Also email me for questions.\n",
        "\n",
        "# How good is good enough?\n",
        "\n",
        "You should aim for an accuracy of at least 75%.  But the higher, the better.  [State-of-the-art accuracy is 91%](https://paperswithcode.com/sota/image-classification-on-cifar-100) The exercise should take 1-3 hours.\n",
        "\n",
        "\n",
        "# Enable GPU\n",
        "\n",
        "You can enable GPU for this notebook via:\n",
        "\n",
        "*Edit -> Notebook Settings -> Hardware Accelerator -> GPU*\n",
        "\n",
        "\n",
        "# Contact\n",
        "\n",
        "For questions contact Jamie Sherrah, jamie.sherrah@adelaide.edu.au\n",
        "\n",
        "June 2019"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcMMlC47WTj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO1q9ve8WZ_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseDataLoader(DataLoader):\n",
        "    \"\"\"\n",
        "    Base class for all data loaders. Provides functionality to split into\n",
        "    train/validation data.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle, validation_split, num_workers,\n",
        "                 collate_fn=default_collate):\n",
        "        self.validation_split = validation_split\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.batch_idx = 0\n",
        "        self.n_samples = len(dataset)\n",
        "\n",
        "        self.sampler, self.valid_sampler = self._split_sampler(self.validation_split)\n",
        "\n",
        "        self.init_kwargs = {\n",
        "            'dataset': dataset,\n",
        "            'batch_size': batch_size,\n",
        "            'shuffle': self.shuffle,\n",
        "            'collate_fn': collate_fn,\n",
        "            'num_workers': num_workers\n",
        "        }\n",
        "        super().__init__(sampler=self.sampler, **self.init_kwargs)\n",
        "\n",
        "    def _split_sampler(self, split):\n",
        "        if split == 0.0:\n",
        "            return None, None\n",
        "\n",
        "        idx_full = np.arange(self.n_samples)\n",
        "\n",
        "        np.random.seed(0)\n",
        "        np.random.shuffle(idx_full)\n",
        "\n",
        "        len_valid = int(self.n_samples * split)\n",
        "\n",
        "        valid_idx = idx_full[0:len_valid]\n",
        "        train_idx = np.delete(idx_full, np.arange(0, len_valid))\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        # turn off shuffle option which is mutually exclusive with sampler\n",
        "        self.shuffle = False\n",
        "        self.n_samples = len(train_idx)\n",
        "\n",
        "        return train_sampler, valid_sampler\n",
        "\n",
        "    def split_validation(self):\n",
        "        if self.valid_sampler is None:\n",
        "            return None\n",
        "        else:\n",
        "            return DataLoader(sampler=self.valid_sampler, **self.init_kwargs)\n",
        "          \n",
        "\n",
        "class Cifar10DataLoader(BaseDataLoader):\n",
        "    \"\"\"\n",
        "    CIFAR10 data loading demo using BaseDataLoader\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir, batch_size, shuffle, validation_split, num_workers, training=True):\n",
        "        trsfm = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.data_dir = data_dir\n",
        "        self.dataset = datasets.CIFAR10(self.data_dir, train=training, download=True, transform=trsfm)\n",
        "        super().__init__(self.dataset, batch_size, shuffle, validation_split, num_workers)\n",
        "\n",
        "\n",
        "class Cifar100DataLoader(BaseDataLoader):\n",
        "    \"\"\"\n",
        "    CIFAR10 data loading demo using BaseDataLoader\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir, batch_size, shuffle, validation_split, num_workers, training=True):\n",
        "        trsfm = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.data_dir = data_dir\n",
        "        self.dataset = datasets.CIFAR100(self.data_dir, train=training, download=True, transform=trsfm)\n",
        "        super().__init__(self.dataset, batch_size, shuffle, validation_split, num_workers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0ZhJwXDWxYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet18(nn.Module):\n",
        "    \"\"\"\n",
        "    Pretrained ResNet18.\n",
        "    \"\"\"\n",
        "  \n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.encoder = models.resnet18(pretrained=False)\n",
        "        num_feats = self.encoder.fc.out_features\n",
        "        self.classifier = nn.Sequential(nn.Linear(num_feats, num_classes))\n",
        "\n",
        "        # Init of last layer\n",
        "        for m in self.classifier:\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3BOuC9vW_1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "    \"\"\"\n",
        "    This class is responsible for performing a full training session.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, loss, metrics, optimizer, epochs, device,\n",
        "                 data_loader, valid_data_loader=None, lr_scheduler=None):\n",
        "        \n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.loss = loss\n",
        "        self.metrics = metrics\n",
        "        self.optimizer = optimizer\n",
        "        self.epochs = epochs\n",
        "        self.start_epoch = 1\n",
        "        \n",
        "        self.data_loader = data_loader\n",
        "        self.valid_data_loader = valid_data_loader\n",
        "        self.do_validation = self.valid_data_loader is not None\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.log_step = int(np.sqrt(data_loader.batch_size))\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Full training logic\n",
        "        \"\"\"\n",
        "        print('Starting training...')\n",
        "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
        "            result = self._train_epoch(epoch)\n",
        "\n",
        "            # save logged informations into log dict\n",
        "            log = {'epoch': epoch}\n",
        "            for key, value in result.items():\n",
        "                if key == 'val_metrics':\n",
        "                    log.update({\n",
        "                        'val_' + mtr.__name__: value[i] for i, mtr in enumerate(self.metrics)})\n",
        "                else:\n",
        "                    log[key] = value\n",
        "\n",
        "            # print logged informations to the screen\n",
        "            for key, value in log.items():\n",
        "                print(f'{str(key):15s}: {value}')\n",
        "\n",
        "    def _train_epoch(self, epoch):\n",
        "        \"\"\"\n",
        "        Training logic for an epoch\n",
        "\n",
        "        :param epoch: Current training epoch.\n",
        "        :return: A log that contains all information you want to save.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        total_metrics = np.zeros(len(self.metrics))\n",
        "        for batch_idx, (data, target) in enumerate(self.data_loader):\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "            loss = self.loss(output, target)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_metrics += self._eval_metrics(output, target)\n",
        "\n",
        "            if batch_idx % self.log_step == 0:\n",
        "                self._log_batch(epoch, batch_idx, self.data_loader.batch_size,\n",
        "                                self.data_loader.n_samples, len(self.data_loader), loss.item())\n",
        "\n",
        "        log = {\n",
        "            'loss': total_loss / len(self.data_loader),\n",
        "            'metrics': (total_metrics / len(self.data_loader)).tolist()\n",
        "        }\n",
        "\n",
        "        if self.do_validation:\n",
        "            val_log = self._valid_epoch(epoch)\n",
        "            log = {**log, **val_log}\n",
        "\n",
        "        if self.lr_scheduler is not None:\n",
        "            self.lr_scheduler.step()\n",
        "\n",
        "        return log\n",
        "\n",
        "    def _log_batch(self, epoch, batch_idx, batch_size, n_samples, len_data, loss):\n",
        "        n_complete = batch_idx * batch_size\n",
        "        percent = 100.0 * batch_idx / len_data\n",
        "        msg = f'Train Epoch: {epoch} [{n_complete}/{n_samples} ({percent:.0f}%)] Loss: {loss:.6f}'\n",
        "        print(msg)\n",
        "\n",
        "    def _eval_metrics(self, output, target):\n",
        "        acc_metrics = np.zeros(len(self.metrics))\n",
        "        for i, metric in enumerate(self.metrics):\n",
        "            acc_metrics[i] += metric(output, target)\n",
        "        return acc_metrics\n",
        "        \n",
        "    def _valid_epoch(self, epoch):\n",
        "        \"\"\"\n",
        "        Validate after training an epoch\n",
        "\n",
        "        :return: A log that contains information about validation\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        total_val_loss = 0\n",
        "        total_val_metrics = np.zeros(len(self.metrics))\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(self.valid_data_loader):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.loss(output, target)\n",
        "                total_val_loss += loss.item()\n",
        "                total_val_metrics += self._eval_metrics(output, target)\n",
        "\n",
        "        return {\n",
        "            'val_loss': total_val_loss / len(self.valid_data_loader),\n",
        "            'val_metrics': (total_val_metrics / len(self.valid_data_loader)).tolist()\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5REY3P8XAb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Runner:\n",
        "    \"\"\"\n",
        "    Top level class to initialise everything and begin training.\n",
        "    \"\"\"\n",
        "  \n",
        "    def train(self, config):\n",
        "        self._seed_everything(config['seed'])\n",
        "\n",
        "        print('Getting data_loader instance')\n",
        "        data_loader = Cifar100DataLoader(**config['data_loader'])\n",
        "        valid_data_loader = data_loader.split_validation()\n",
        "\n",
        "        print('Building model architecture')\n",
        "        model = ResNet18(num_classes=100)\n",
        "        model, device = self._prepare_device(model, config['n_gpu'])\n",
        "\n",
        "        print('Getting loss and metric function handles')\n",
        "        loss = nn.CrossEntropyLoss()\n",
        "        metrics = [\n",
        "          # funcs go here\n",
        "        ]\n",
        "\n",
        "        print('Building optimizer and lr scheduler')\n",
        "        trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "        optimizer = torch.optim.SGD(trainable_params, lr=0.001)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=2)\n",
        "\n",
        "        print('Initialising trainer')\n",
        "        trainer = Trainer(model, loss, metrics, optimizer,\n",
        "                        epochs=config['training']['epochs'],\n",
        "                        device=device,\n",
        "                        data_loader=data_loader,\n",
        "                        valid_data_loader=valid_data_loader,\n",
        "                        lr_scheduler=lr_scheduler)\n",
        "\n",
        "        trainer.train()\n",
        "        print('Finished!')\n",
        "\n",
        "    def _prepare_device(self, model, n_gpu_use):\n",
        "        device, device_ids = self._get_device(n_gpu_use)\n",
        "        model = model.to(device)\n",
        "        if len(device_ids) > 1:\n",
        "            model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
        "        return model, device\n",
        "\n",
        "    def _get_device(self, n_gpu_use):\n",
        "        \"\"\"\n",
        "        setup GPU device if available, move model into configured device\n",
        "        \"\"\"\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "        if n_gpu_use > 0 and n_gpu == 0:\n",
        "            print(\"Warning: There\\'s no GPU available on this machine,\"\n",
        "                  \"training will be performed on CPU.\")\n",
        "            n_gpu_use = 0\n",
        "        if n_gpu_use > n_gpu:\n",
        "            print(f\"Warning: The number of GPU\\'s configured to use is {n_gpu_use}, \"\n",
        "                  f\"but only {n_gpu} are available on this machine.\")\n",
        "            n_gpu_use = n_gpu\n",
        "        device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n",
        "        list_ids = list(range(n_gpu_use))\n",
        "        print(f'Using device: {device}, {list_ids}')\n",
        "        return device, list_ids\n",
        "\n",
        "    def _seed_everything(self, seed):\n",
        "        print(f'Using random seed: {seed}')\n",
        "        random.seed(seed)\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chRLtbSiXAoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'name': 'CIFAR_Demo',\n",
        "    'n_gpu': 1,                 # use GPU if available\n",
        "    'seed': 1234,               # random seed to use for everything\n",
        "    \n",
        "    'data_loader': {\n",
        "      'batch_size': 32,        # training batch size\n",
        "      'data_dir': 'data',       # directory to download dataset to\n",
        "      'num_workers': 2,         # data loading parallelisation\n",
        "      'shuffle': 'False',        # shuffle training samples\n",
        "      'validation_split': 0.1   # split of data to use for validation\n",
        "    },\n",
        "    \n",
        "    'training': {\n",
        "      'epochs': 1              # N training epochs\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXp8oshrXAyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Runner().train(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jzEpxlfn1G-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}